by: eric xing

### Trap of qualifying intelligence (turing trap)

*lingual* intelligence vs *general* intelligence

**purpose** as a driver of intelligence 
 -> kinda weird though, i know intelligent people without purpose

Language cannot express everything (doubt)
 - not the most efficient medium

How to build a world model
From *next word* prediction to *next world*

A world model is all you need

All human behavior come from simulating world models, and evaluating their good and bad aspects. That's what intelligence is. Simulating shit.

#### Reasoning vs simulation

Hypothetical reasoning, thinking in simulations.

**Bayesian shit**

World models can be transfered from one similar task, and world, to another. We can combine and associate worlds.



PAN (physical, agentic,nested) world

### Proposed framework

Put everything into latent space, and lossify there, to build a cool model

### Critiques

Discret vs continuous operations
 - Analog ? 

Representation and interpretability:
 - bad intepretability of learned models

**Completeness of language**:
 - funny

Get rid of autoregressive and use *JEPA* arch

Basically, do autoregession in the embedding space instead of the output *real* space. 
Predict embedding instead of tokens.

**Latent Loss** -> surrogate of the observation loss
MPC training (better than RL)

### Agents

Current agents are stupid
 A real agents is a system that can actually perceive, reason, and learn to adapt. 

Nested world models :
 - world models of world models, ..... infinetely recursive 
Agents are decision systems interacting with simulated worlds in a world model.
 
### Super agents and super world models

Kinda like this black mirror episode with the dating app

A big agents that aggregates sub agents behaviors to take decisions. Basically big world simulations. 
 
Sub Agent goals ?

Agentic != system with agency

How could we define free will as a trainable task ? 
Doesnt even exist imo !

"Battlefield tactics" -> mf knows where he is talking


Is their scientific data, or experimental results on the actual limotations of world models ?

as they try to predict the world, and especially other agent behavior, it can quicly get *chaotic*. 
Is world model convergence in multi agent systems sometimes proved, or at least observed empirically ?




